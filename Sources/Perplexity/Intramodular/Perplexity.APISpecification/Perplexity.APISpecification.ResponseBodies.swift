//
// Copyright (c) Vatsal Manot
//

import Foundation
import OpenAI

extension Perplexity.APISpecification.ResponseBodies {
    public struct ChatCompletion: Codable, Hashable, Sendable {
        public struct Choice: Codable, Hashable, Sendable {
            public enum FinishReason: String, Codable, Hashable, Sendable {
                case stop
                case length
            }
            
            public let index: Int
            
            /// The message generated by the model.
            public let message: Perplexity.ChatMessage
            
            /// The reason the model stopped generating tokens. Possible values include stop if the model hit a natural stopping point, or length if the maximum number of tokens specified in the request was reached.
            public let finishReason: FinishReason
            
            /// The incrementally streamed next tokens. Only meaningful when stream = true.
            public let delta: Perplexity.ChatMessage
        }
        
        public struct Usage: Codable, Hashable, Sendable {
            public let promptTokens: Int
            public let completionTokens: Int
            public let totalTokens: Int
        }
        
        /// An ID generated uniquely for each response.
        public var id: String
        public var object: String
        public var created: Date
        public var model: Perplexity.Model
        public var choices: [Choice]
        public let usage: Usage
    }
}
